import json
import os
import subprocess
import sys
import concurrent.futures
import time
import re
import shutil
from datetime import datetime

# --- é…ç½®åŒºåŸŸ ---
CONFIG_FILE = 'rules.json'
MAX_WORKERS = 5
GITHUB_STEP_SUMMARY = os.getenv('GITHUB_STEP_SUMMARY')

# ç»Ÿä¸€è¾“å‡ºç›®å½• (æ‰€æœ‰äº§ç‰©éƒ½æ”¾è¿™é‡Œ)
DIR_OUTPUT = "rules"

# ä¸¥æ ¼æ˜ å°„è¡¨
RULE_MAP = {
    'DOMAIN-SUFFIX': 'domain_suffix',
    'HOST-SUFFIX': 'domain_suffix',
    'DOMAIN': 'domain',
    'HOST': 'domain',
    'DOMAIN-KEYWORD': 'domain_keyword',
    'HOST-KEYWORD': 'domain_keyword',
    'IP-CIDR': 'ip_cidr',
    'IP-CIDR6': 'ip_cidr',
    'SRC-IP-CIDR': 'source_ip_cidr',
    'GEOIP': 'geoip',
    'DST-PORT': 'port',
    'SRC-PORT': 'source_port',
    'PROCESS-NAME': 'process_name'
}

class TaskResult:
    def __init__(self, name, status, msg, size="0KB"):
        self.name = name
        self.status = status
        self.msg = msg
        self.size = size

def setup_directories():
    """åˆå§‹åŒ–ç›®å½•"""
    if not os.path.exists(DIR_OUTPUT):
        os.makedirs(DIR_OUTPUT)

def get_core_version():
    core_path = "./sing-box"
    if not os.path.exists(core_path): return "âŒ æ ¸å¿ƒç¼ºå¤±"
    try:
        result = subprocess.run([core_path, "version"], capture_output=True, text=True, check=True)
        return result.stdout.split('\n')[0].split('version ')[-1].strip()
    except: return "â“ æœªçŸ¥ç‰ˆæœ¬"

def get_file_size(filepath):
    if not os.path.exists(filepath): return "0KB"
    size = os.path.getsize(filepath)
    for unit in ['B', 'KB', 'MB']:
        if size < 1024: return f"{size:.1f}{unit}"
        size /= 1024
    return f"{size:.1f}GB"

def download_file(url, filename):
    ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    cmd = ["curl", "-L", "--fail", "--retry", "3", "-A", ua, url, "-o", filename]
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError:
        return False

# --- æ·±åº¦ä¼˜åŒ– JSON ---
def optimize_json_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        rules = data.get('rules', [])
        total_removed = 0
        modified = False

        for rule in rules:
            keys_to_remove = []
            for key, val in rule.items():
                if isinstance(val, list):
                    new_val = sorted(list(set(val)))
                    removed_count = len(val) - len(new_val)
                    if removed_count > 0:
                        rule[key] = new_val
                        total_removed += removed_count
                        modified = True
                    if len(new_val) == 0:
                        keys_to_remove.append(key)
                        modified = True
            for k in keys_to_remove:
                del rule[k]
        
        if modified:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            return True, total_removed
        return False, 0
    except Exception as e:
        print(f"âš ï¸ ä¼˜åŒ– JSON å¤±è´¥: {e}")
        return False, 0

# --- è½¬æ¢å™¨ ---
def convert_clash_to_json(input_file, output_json):
    rules_dict = {v: set() for v in set(RULE_MAP.values())}
    count = 0
    try:
        with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#') or line.startswith('//'): continue
            line = re.split(r'\s*(#|//)', line)[0].strip()
            
            match = re.search(r'^([A-Z0-9-]+)\s*,\s*([^,]+)', line, re.IGNORECASE)
            if match:
                raw_type = match.group(1).upper()
                value = match.group(2).strip().strip("'\"")
                if raw_type in RULE_MAP:
                    sb_type = RULE_MAP[raw_type]
                    rules_dict[sb_type].add(value)
                    count += 1

        if count == 0: return False, "æ— æœ‰æ•ˆè§„åˆ™"

        final_rules = []
        for k, v in rules_dict.items():
            if v: final_rules.append({k: sorted(list(v))})
        
        output_data = {"version": 3, "rules": final_rules}
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        return True, f"è½¬æ¢{count}æ¡"
    except Exception as e:
        return False, f"å¼‚å¸¸: {str(e)}"

# --- ç¼–è¯‘ç»„ä»¶ ---
def decompile_srs(input_srs, output_json):
    cmd = ["./sing-box", "rule-set", "decompile", input_srs, "-o", output_json]
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        if os.path.getsize(output_json) < 10: return False
        return True
    except subprocess.CalledProcessError:
        return False

def compile_json(input_json, output_srs):
    cmd = ["./sing-box", "rule-set", "compile", input_json, "-o", output_srs]
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError:
        return False

def process_single_task(name, url):
    print(f"ğŸ”„ [{name}] å¯åŠ¨å¤„ç†...")
    
    # ä¸´æ—¶ä¸‹è½½è·¯å¾„ (æ ¹ç›®å½•ä¸´æ—¶)
    temp_download = f"temp_raw_{name}"
    # æœ€ç»ˆè¾“å‡ºè·¯å¾„ (å…¨éƒ¨æ”¾å…¥ rules æ–‡ä»¶å¤¹)
    final_json = os.path.join(DIR_OUTPUT, f"{name}.json")
    final_srs = os.path.join(DIR_OUTPUT, f"{name}.srs")
    
    if not download_file(url, temp_download):
        return TaskResult(name, "âŒ", "ä¸‹è½½å¤±è´¥")
    
    url_lower = url.lower()
    process_info = "æœªçŸ¥"
    json_ready = False
    
    try:
        if url_lower.endswith('.srs'):
            print(f"ğŸ›¡ï¸ [{name}] éªŒè¯ SRS...")
            if decompile_srs(temp_download, final_json):
                process_info = "SRSé‡æ„"
                json_ready = True
            else:
                return TaskResult(name, "âŒ", "SRSéªŒè¯å¤±è´¥")
                
        elif url_lower.endswith('.json'):
            shutil.move(temp_download, final_json)
            process_info = "JSONåŸç”Ÿ"
            json_ready = True
            
        elif url_lower.endswith('.mrs'):
            return TaskResult(name, "âŒ", "ä¸æ”¯æŒMRS")
            
        else:
            print(f"ğŸ”§ [{name}] è½¬æ¢æ ¼å¼...")
            success, msg = convert_clash_to_json(temp_download, final_json)
            if success:
                process_info = "æ ¼å¼è½¬æ¢"
                json_ready = True
            else:
                return TaskResult(name, "âŒ", f"è§£æå¤±è´¥: {msg}")

    except Exception as e:
         return TaskResult(name, "âŒ", f"å¼‚å¸¸: {str(e)}")
    finally:
        if os.path.exists(temp_download): os.remove(temp_download)

    if json_ready:
        is_opt, opt_count = optimize_json_file(final_json)
        if is_opt:
            process_info += f"(å»é‡{opt_count})"

        if compile_json(final_json, final_srs):
            size = get_file_size(final_srs)
            print(f"âœ… [{name}] æˆåŠŸ: {process_info}")
            return TaskResult(name, "âœ…", f"{process_info}", size)
        else:
            print(f"âŒ [{name}] ç¼–è¯‘æ‹’ç»")
            return TaskResult(name, "âŒ", "ç¼–è¯‘æ‹’ç»(JSONéæ³•)")
            
    return TaskResult(name, "âŒ", "é€»è¾‘é”™è¯¯")

# --- æ–°å¢æ”¹è¿›ï¼šç”Ÿæˆ rules æ–‡ä»¶å¤¹çš„ Readme ---
def generate_folder_readme(results, core_ver):
    readme_path = os.path.join(DIR_OUTPUT, "README.md")
    success_results = [r for r in results if r.status == "âœ…"]
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(f"# ğŸ“¦ Rule Sets Collection\n\n")
        f.write(f"> **Core Version**: `{core_ver}`\n")
        f.write(f"> **Last Update**: `{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC)`\n\n")
        f.write("| Rule Name | SRS File | Source JSON | Size | Details |\n")
        f.write("| :--- | :--- | :--- | :--- | :--- |\n")
        
        for r in success_results:
            # ç”Ÿæˆç›¸å¯¹é“¾æ¥
            srs_link = f"[{r.name}.srs]({r.name}.srs)"
            json_link = f"[{r.name}.json]({r.name}.json)"
            f.write(f"| **{r.name}** | {srs_link} | {json_link} | {r.size} | {r.msg} |\n")

def write_summary(results, core_ver):
    if not GITHUB_STEP_SUMMARY: return
    success_cnt = sum(1 for r in results if r.status == "âœ…")
    fail_cnt = len(results) - success_cnt
    with open(GITHUB_STEP_SUMMARY, 'a', encoding='utf-8') as f:
        f.write(f"## ğŸ­ è§„åˆ™å·¥å‚æŠ¥å‘Š (Clean Layout)\n")
        f.write(f"- **æ ¸å¿ƒ**: `{core_ver}`\n")
        f.write(f"- **ç»Ÿè®¡**: âœ… {success_cnt} | âŒ {fail_cnt}\n")
        f.write(f"> ğŸ“‚ æ‰€æœ‰äº§ç‰©å·²æ”¶çº³è‡³ `{DIR_OUTPUT}/` æ–‡ä»¶å¤¹ã€‚\n\n")
        f.write("| è§„åˆ™ | çŠ¶æ€ | è¯¦æƒ… | å¤§å° |\n|:---|:---:|:---|:---:|\n")
        for r in results: f.write(f"| {r.name} | {r.status} | {r.msg} | {r.size} |\n")

def main():
    print("ğŸš€ å¯åŠ¨ Sing-box å…¨èƒ½å·¥å‚ (Clean Edition)")
    setup_directories()
    core_ver = get_core_version()
    print(f"ğŸ’ æ ¸å¿ƒ: {core_ver}")
    if "âŒ" in core_ver: sys.exit(1)

    tasks = {}
    if len(sys.argv) == 3:
        tasks[sys.argv[1]] = sys.argv[2]
    elif os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r') as f:
                content = f.read().strip()
                if content: tasks = json.loads(content)
        except: pass

    if not tasks:
        print("â„¹ï¸ æ— ä»»åŠ¡")
        return

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_single_task, n, u): n for n, u in tasks.items()}
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())

    # ç”Ÿæˆä¸¤ç§æŠ¥å‘Š
    generate_folder_readme(results, core_ver)
    write_summary(results, core_ver)
    
    if all(r.status == "âŒ" for r in results): sys.exit(1)

if __name__ == "__main__":
    main()
